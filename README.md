# Project Overview

This project is designed to work with embeddings, text extraction, and retrieval pipelines, leveraging vector databases and language models. It provides a modular architecture to process documents, generate embeddings, store and retrieve them efficiently, and interact with users through a UI.

## File Descriptions

- **imports.py**  
  This file centralizes all external library imports and may include wrapper classes or helper functions to standardize usage of third-party APIs and models across the project.

- **requirements.txt**  
  Lists all Python dependencies required to run the project, including libraries for embeddings, vector databases, PDF processing, and UI frameworks.

- **file_to_text.py**  
  Contains functions to extract text from various file formats, primarily PDFs. This enables converting documents into plain text for embedding generation and further processing.

- **database.py**  
  Manages the connection to the Chroma vector database. Provides functions such as `store_embedding` to add embeddings with associated keys and persist the database state.

- **embeddings.py**  
  Defines the `get_embedding` function that generates vector embeddings from input text using the Ollama embedding model. Handles errors and returns embedding vectors for downstream use.

- **Retriever.py**  
  Implements retrieval logic to query the vector database based on input queries. Likely includes functions to fetch the most relevant documents or text chunks by similarity search.

- **Prompt_template_for_LLM.py**  
  Contains prompt templates and formatting utilities for interacting with large language models (LLMs). Helps standardize prompts for consistent and effective LLM responses.

- **RAG_pipeline.py**  
  Implements the Retrieval-Augmented Generation (RAG) pipeline, combining retrieval from the vector database with generation capabilities of LLMs to answer queries or generate content based on retrieved context.

- **GradioUI.py**  
  Provides a Gradio-based web user interface for the project. Allows users to input queries, upload documents, and receive responses generated by the RAG pipeline and embeddings system.

- **test_embeddings.py**  
  Contains test cases for the embeddings functionality. Tests embedding generation with various input types and stores embeddings in the database, printing results for verification.

## Project Architecture

The project follows a modular design:

1. **Text Extraction:** Documents are converted to text using `file_to_text.py`.
2. **Embedding Generation:** Text is converted to vector embeddings via `embeddings.py`.
3. **Storage:** Embeddings are stored in a persistent vector database managed by `database.py`.
4. **Retrieval:** Queries are processed by `Retriever.py` to find relevant embeddings.
5. **Generation:** The RAG pipeline (`RAG_pipeline.py`) uses retrieved context to generate answers with LLMs.
6. **User Interface:** `GradioUI.py` provides an interactive frontend for users.

## Setup and Usage

1. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

2. Ensure Ollama is installed and running locally, as it is required for the embedding model to function properly.  
   Download and installation instructions: https://ollama.com/download

3. Run tests for embeddings:

   ```bash
   python test_embeddings.py
   ```

4. Start the Gradio UI:

   ```bash
   python GradioUI.py
   ```

5. Batch process PDFs in the data folder to generate and store embeddings:

   ```bash
   python text_chunk.py
   ```

   This will process all PDF files in the `data` directory, generate embeddings for their text content, and store them in the vector database.
   python GradioUI.py
   python test_embeddings.py

## Notes

- The project depends on the Ollama embedding model, which requires the Ollama service to be running locally.
- The vector database uses Chroma with persistence in the `./chroma_db` directory.
- The RAG pipeline integrates retrieval and generation for enhanced question answering.
- Proper setup of Ollama is critical for embedding generation and overall functionality.

## Additional Information

- The `text_chunk.py` script allows batch processing of all PDF files in the `data` folder. It extracts text, generates embeddings, and stores them in the vector database automatically.
- The embedding model used is `"granite-embedding:278m"` from Ollama, and the LLM model used in the RAG pipeline is `"granite3.1-moe:1b"`.
- The project uses Langchain components extensively for text splitting, document loading, and chain creation.
- The Gradio UI provides a streaming chat interface to interact with the RAG pipeline, allowing users to ask questions about the processed documents.
- Ensure that the `./chroma_db` directory has appropriate read/write permissions for persistence.
- For troubleshooting embedding generation issues, verify that the Ollama service is running and accessible as per the instructions at https://ollama.com/download.
